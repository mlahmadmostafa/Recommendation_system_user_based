{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_description</th>\n",
       "      <th>relevance_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>simpson strongtie gauge angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3.0</td>\n",
       "      <td>not only do angles make joints stronger they a...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>simpson strongtie gauge angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>2.5</td>\n",
       "      <td>not only do angles make joints stronger they a...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                  product_title    search_term  relevance  \\\n",
       "0   2       100001  simpson strongtie gauge angle  angle bracket        3.0   \n",
       "1   3       100001  simpson strongtie gauge angle      l bracket        2.5   \n",
       "\n",
       "                                 product_description  relevance_normalized  \n",
       "0  not only do angles make joints stronger they a...                  1.00  \n",
       "1  not only do angles make joints stronger they a...                  0.75  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'^@|^http|[^a-zA-Z\\s]', '', str(text))\n",
    "\n",
    "df1 = pd.read_csv('train.csv',encoding='latin1')\n",
    "df2 = pd.read_csv('product_descriptions.csv')\n",
    "df = pd.merge(df1, df2, on=\"product_uid\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "df[\"product_title\"] = df[\"product_title\"].apply(remove_special_chars).str.lower()\n",
    "df[\"product_description\"] = df[\"product_description\"].apply(remove_special_chars).str.lower()\n",
    "df[\"search_term\"] = df[\"search_term\"].apply(remove_special_chars).str.lower()\n",
    "df[\"relevance_normalized\"] = (df[\"relevance\"] - df[\"relevance\"].min())/(df[\"relevance\"].max() - df[\"relevance\"].min())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "r = Rake()\n",
    "def extract_keywords(text, max_keywords=20):\n",
    "    r.extract_keywords_from_text(text)\n",
    "    keywords = []\n",
    "    for phrase in r.get_ranked_phrases():\n",
    "        keywords.extend(phrase.split())  # Split the phrase into individual words\n",
    "\n",
    "    # Return the top `max_keywords` words (limiting to `max_keywords` words)\n",
    "    return keywords[:max_keywords]\n",
    "\n",
    "df[\"product_description_keywords\"] = df[\"product_description\"].apply(\n",
    "    lambda desc: extract_keywords(desc)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer_title, tokenizer_description, tokenizer_search_term, sequences_title, sequences_description, sequences_search_term = pickle.load(handle)\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import pickle\n",
    "\n",
    "# tokenizer_title = Tokenizer()\n",
    "# tokenizer_title.fit_on_texts(df[\"product_title\"])\n",
    "# tokenizer_description = Tokenizer()\n",
    "# tokenizer_description.fit_on_texts(df[\"product_description_keywords\"])\n",
    "# tokenizer_search_term = Tokenizer()\n",
    "# tokenizer_search_term.fit_on_texts(df[\"search_term\"])\n",
    "\n",
    "# sequences_title =       pad_sequences(tokenizer_title.texts_to_sequences(df[\"product_title\"]), padding='post')\n",
    "# sequences_description = pad_sequences(tokenizer_description.texts_to_sequences(df[\"product_description_keywords\"]), padding='post')\n",
    "# sequences_search_term = pad_sequences(tokenizer_search_term.texts_to_sequences(df[\"search_term\"]), padding='post')\n",
    "\n",
    "# with open('tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump([tokenizer_title, tokenizer_description, tokenizer_search_term,  \n",
    "#                  sequences_title, sequences_description, sequences_search_term], \n",
    "#                 handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec.pickle', 'rb') as handle:\n",
    "    word2vec_model = pickle.load(handle)\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "# with open('word2vec.pickle', 'wb') as handle:\n",
    "#     pickle.dump(word2vec_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab_len = len(tokenizer_title.word_index)\n",
    "embedding_matrix_title = np.empty((len(tokenizer_title.word_index), 300), dtype=np.float16)\n",
    "for word, i in tokenizer_title.word_index.items():\n",
    "    if i-1 < len(embedding_matrix_title):  # Ensure index is within bounds\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix_title[i - 1] = word2vec_model[word][:300]\n",
    "        else:\n",
    "            embedding_matrix_title[i - 1] = np.random.normal(size=300)  # Optional: handle missing words\n",
    "    else:\n",
    "        print(f\"Index {i-1} is out of bounds for word: {word}\")\n",
    "\n",
    "embedding_matrix_description =  np.empty((len(tokenizer_description.word_index), 300), dtype=np.float16)\n",
    "for word, i in tokenizer_description.word_index.items():\n",
    "    if i-1 < len(embedding_matrix_description):  # Ensure index is within bounds\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix_description[i - 1] = word2vec_model[word][:300]\n",
    "        else:\n",
    "            embedding_matrix_description[i - 1] = np.random.normal(size=300)  # Optional: handle missing words\n",
    "    else:\n",
    "        print(f\"Index {i-1} is out of bounds for word: {word}\")        \n",
    "        \n",
    "embedding_matrix_search_term =  np.empty((len(tokenizer_search_term.word_index), 300), dtype=np.float16)\n",
    "for word, i in tokenizer_search_term.word_index.items():\n",
    "    if i-1 < len(embedding_matrix_search_term):  # Ensure index is within bounds\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix_search_term[i - 1] = word2vec_model[word][:300]\n",
    "        else:\n",
    "            embedding_matrix_search_term[i - 1] = np.random.normal(size=300)  # Optional: handle missing words\n",
    "    else:\n",
    "        print(f\"Index {i-1} is out of bounds for word: {word}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input,Concatenate,Dropout,GlobalMaxPooling1D,BatchNormalization\n",
    "import tensorflow as tf\n",
    "def create_text_submodel_lstm(input_layer, vocab_size, embedding_dim, embedding_matrix, lstm_units=100):\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix] if embedding_matrix is not None else None,\n",
    "        trainable=False\n",
    "    )(input_layer)\n",
    "    # if embedding_matrix is not None:\n",
    "    #     embedding.build((None,))  # Build the layer\n",
    "    #     embedding.set_weights([embedding_matrix])\n",
    "    lstm = LSTM(lstm_units, activation='tanh')(embedding)\n",
    "    output = Dense(1, activation='relu')(lstm)\n",
    "    return output\n",
    "\n",
    "def create_text_submodel(input_layer, vocab_size, embedding_dim, embedding_matrix):\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix] if embedding_matrix is not None else None,\n",
    "        trainable=False\n",
    "    )(input_layer)\n",
    "    flattened = GlobalMaxPooling1D()(embedding)\n",
    "    # Replace LSTM with pooling/dense layers\n",
    "    \n",
    "    dense0 = Dense(512, activation='relu')(flattened)\n",
    "    norm0 = BatchNormalization()(dense0)\n",
    "    dropout0 = Dropout(0.3)(norm0)\n",
    "    dense1 = Dense(256, activation='relu')(dropout0)\n",
    "    norm1 = BatchNormalization()(dense1)\n",
    "    dropout1 = Dropout(0.3)(norm1)\n",
    "    dense2 = Dense(128, activation='relu')(dropout1)\n",
    "    norm2 = BatchNormalization()(dense2)\n",
    "    dropout2 = Dropout(0.3)(norm2)\n",
    "    dense3 = Dense(64, activation='relu')(dropout1)\n",
    "    norm3 = BatchNormalization()(dense3)\n",
    "    dropout3 = Dropout(0.3)(norm3)\n",
    "    output = Dense(1, activation='relu')(dropout3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightedEnsemble(tf.keras.layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(WeightedEnsemble, self).__init__(**kwargs)\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         # Assuming inputs are [title_output, desc_output, search_output]\n",
    "#         # You might want to add weights or a more sophisticated combination method\n",
    "#         return tf.reduce_mean(inputs, axis=0)\n",
    "\n",
    "def create_relevance_model(sequences_title, sequences_description, sequences_search_term,\n",
    "                         tokenizer_title, tokenizer_description, tokenizer_search_term,\n",
    "                         embedding_matrix_title, embedding_matrix_description, embedding_matrix_search_term,\n",
    "                         embedding_dim=300):\n",
    "    # Input layers\n",
    "    title_input = Input(shape=(sequences_title.shape[1],), name='title_input')\n",
    "    desc_input = Input(shape=(sequences_description.shape[1],), name='description_input')\n",
    "    search_input = Input(shape=(sequences_search_term.shape[1],), name='search_input')\n",
    "    \n",
    "    title_output = create_text_submodel(\n",
    "        title_input,\n",
    "        vocab_size=len(tokenizer_title.word_index),\n",
    "        embedding_dim=embedding_dim,\n",
    "        embedding_matrix=embedding_matrix_title,\n",
    "\n",
    "    )\n",
    "    \n",
    "    desc_output = create_text_submodel(\n",
    "        desc_input,\n",
    "        vocab_size=len(tokenizer_description.word_index),\n",
    "        embedding_dim=embedding_dim,\n",
    "        embedding_matrix=embedding_matrix_description,\n",
    "\n",
    "    )\n",
    "    \n",
    "    search_output = create_text_submodel(\n",
    "        search_input,\n",
    "        vocab_size=len(tokenizer_search_term.word_index),\n",
    "        embedding_dim=embedding_dim,\n",
    "        embedding_matrix=embedding_matrix_search_term,\n",
    "\n",
    "    )\n",
    "    \n",
    "\n",
    "    combined_output = Concatenate()([title_output, desc_output, search_output])\n",
    "    x = Dense(128, activation='relu')(combined_output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(combined_output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create final model\n",
    "    relevance_model = tf.keras.models.Model(\n",
    "        inputs=[title_input, desc_input, search_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    relevance_model.compile(\n",
    "        loss=tf.keras.losses.Huber(),\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return relevance_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',                # Path where the model will be saved\n",
    "    monitor='val_loss',             # Metric to monitor\n",
    "    save_best_only=True,            # Save only the best model\n",
    "    mode='min',                     # 'min' for loss, 'max' for accuracy\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',             \n",
    "    patience=20,                     \n",
    "    mode='min',                     \n",
    "    verbose=0,\n",
    "    restore_best_weights=True      \n",
    ") \n",
    "\n",
    "from tqdm.keras import TqdmCallback\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " title_input (InputLayer)       [(None, 28)]         0           []                               \n",
      "                                                                                                  \n",
      " description_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " search_input (InputLayer)      [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_15 (Embedding)       (None, 28, 300)      5762400     ['title_input[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)       (None, 20, 300)      26555400    ['description_input[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_17 (Embedding)       (None, 11, 300)      2140500     ['search_input[0][0]']           \n",
      "                                                                                                  \n",
      " global_max_pooling1d_9 (Global  (None, 300)         0           ['embedding_15[0][0]']           \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_10 (Globa  (None, 300)         0           ['embedding_16[0][0]']           \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_11 (Globa  (None, 300)         0           ['embedding_17[0][0]']           \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " dense_58 (Dense)               (None, 512)          154112      ['global_max_pooling1d_9[0][0]'] \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 512)          154112      ['global_max_pooling1d_10[0][0]']\n",
      "                                                                                                  \n",
      " dense_68 (Dense)               (None, 512)          154112      ['global_max_pooling1d_11[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 512)         2048        ['dense_58[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 512)         2048        ['dense_63[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 512)         2048        ['dense_68[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 512)          0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 512)          0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 512)          0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dense_59 (Dense)               (None, 256)          131328      ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 256)          131328      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " dense_69 (Dense)               (None, 256)          131328      ['dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 256)         1024        ['dense_59[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 256)         1024        ['dense_64[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 256)         1024        ['dense_69[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 256)          0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 256)          0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 256)          0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 64)           16448       ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 64)           16448       ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 64)           16448       ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 64)          256         ['dense_61[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 64)          256         ['dense_66[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 64)          256         ['dense_71[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 64)           0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 64)           0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)           (None, 64)           0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dense_62 (Dense)               (None, 1)            65          ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dense_67 (Dense)               (None, 1)            65          ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " dense_72 (Dense)               (None, 1)            65          ['dropout_45[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 3)            0           ['dense_62[0][0]',               \n",
      "                                                                  'dense_67[0][0]',               \n",
      "                                                                  'dense_72[0][0]']               \n",
      "                                                                                                  \n",
      " dense_74 (Dense)               (None, 64)           256         ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 64)          256         ['dense_74[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 64)           0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 1)            65          ['dropout_46[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 35,374,720\n",
      "Trainable params: 911,300\n",
      "Non-trainable params: 34,463,420\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BF50EC0D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 256ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0073511 ],\n",
       "       [ 0.00424599],\n",
       "       [-0.00895748]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_relevance_model(sequences_title, sequences_description, sequences_search_term,\n",
    "                               tokenizer_title, tokenizer_description, tokenizer_search_term,\n",
    "                               embedding_matrix_title, embedding_matrix_description, embedding_matrix_search_term)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss=\"mean_squared_error\", metrics=['mae'])\n",
    "model.predict([sequences_title[0:3], sequences_description[0:3], sequences_search_term[0:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105195901b594c7db7413af13e6851ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12b239b4c6d4601a1cce0052ab1e9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf51128ac0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([sequences_title, sequences_description, sequences_search_term]\n",
    "                     , df[\"relevance_normalized\"]\n",
    "                     , validation_split=0.2\n",
    "                     , verbose=0\n",
    "                     , epochs=100, batch_size=32, callbacks=[\n",
    "                                                            checkpoint_callback,\n",
    "                                                            TqdmCallback(verbose=1),\n",
    "                                                            early_stopping_callback\n",
    "                                                            ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74067"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
